{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "5278be9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from inspect import getmro\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix, roc_auc_score, f1_score, calinski_harabasz_score, davies_bouldin_score, recall_score, precision_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from catboost import CatBoostClassifier\n",
    "from operator import itemgetter\n",
    "from itertools import islice\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import statsmodels.api as sm\n",
    "from sklearn.ensemble import AdaBoostClassifier, VotingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from hyperopt import hp, fmin, tpe, Trials, STATUS_OK\n",
    "\n",
    "#import tensorflow as tsf\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#print(f'Found TF-DF: {tsf.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3280d24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataDigest:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.ages = None\n",
    "        self.fares = None\n",
    "        self.titles = None\n",
    "        self.cabins = None\n",
    "        self.families = None\n",
    "        self.tickets = None\n",
    "\n",
    "def get_title(name):\n",
    "    if pd.isnull(name):\n",
    "        return \"Null\"\n",
    "\n",
    "    title_search = re.search(' ([A-Za-z]+)\\.', name)\n",
    "    if title_search:\n",
    "        return title_search.group(1).lower()\n",
    "    else:\n",
    "        return \"None\"\n",
    "    \n",
    "def get_index(item, index):\n",
    "    if pd.isnull(item):\n",
    "        return -1\n",
    "\n",
    "    try:\n",
    "        return index.get_loc(item)\n",
    "    except KeyError:\n",
    "        return -1\n",
    "\n",
    "\n",
    "def get_family(row):\n",
    "    last_name = row[\"Name\"].split(\",\")[0]\n",
    "    if last_name:\n",
    "        family_size = 1 + row[\"Parch\"] + row[\"SibSp\"]\n",
    "        if family_size > 3:\n",
    "            return \"{0}_{1}\".format(last_name.lower(), family_size)\n",
    "        else:\n",
    "            return \"nofamily\"\n",
    "    else:\n",
    "        return \"unknown\"\n",
    "\n",
    "    \n",
    "data_digest = DataDigest()\n",
    "data_digest.ages = all_data.groupby(\"Sex\")[\"Age\"].median()\n",
    "data_digest.fares = all_data.groupby(\"Pclass\")[\"Fare\"].median()\n",
    "data_digest.titles = pd.Index(test[\"Name\"].apply(get_title).unique())\n",
    "data_digest.families = pd.Index(test.apply(get_family, axis=1).unique())\n",
    "data_digest.cabins = pd.Index(test[\"Cabin\"].fillna(\"unknown\").unique())\n",
    "data_digest.tickets = pd.Index(test[\"Ticket\"].fillna(\"unknown\").unique())\n",
    "\n",
    "\n",
    "def preposition(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    def nornalise_name(x):\n",
    "        return \" \".join([v.strip(\",()[].\\\"'\") for v in x.split(\" \")])\n",
    "    \n",
    "    def ticket_number(x):\n",
    "        return x.split(\" \")[-1]\n",
    "    \n",
    "    def ticket_item(x):\n",
    "        item = x.split(\" \")\n",
    "        if len(item) == 1:\n",
    "            return \"None\"\n",
    "        else:\n",
    "            return \"_\".join(item[0:-1])\n",
    "        \n",
    "    df['Name'] = df['Name'].apply(nornalise_name)\n",
    "    df['Ticket_number'] = df['Ticket'].apply(ticket_number)\n",
    "    df['Ticket_item'] = df['Ticket'].apply(ticket_item)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def Preprocessing(train_data):\n",
    "    #Deal with Missing values, for Embarked, fill with the value that occurs the most\n",
    "    #train_data['Cabin'].fillna(\"N0\",inplace=True)\n",
    "    train_data['Age'].fillna(train_data['Age'].mean(),inplace=True)\n",
    "    train_data['Fare'].fillna(train_data['Fare'].mean(),inplace=True)\n",
    "    train_data['Embarked'].fillna(train_data['Embarked'].mode()[0],inplace=True)\n",
    "    train_data['Embarked'] = train_data['Embarked'].astype('str')\n",
    "\n",
    "    #Maybe Cabin have information as which part of the boat the individual was located,and relate to the survival rate, Due to the complexity of how this is logged,we will use the section letter\n",
    "    #train_data['Cabin_Section']=train_data['Cabin'].map(lambda x:x[0])\n",
    "    #The Cabin info in hard to be utilized, meanwhile, train & test data have diff cabin numbers, and there's 75% of missing values in cabin, we will just drop this column\n",
    "    \n",
    "    col_for_lble = ['Sex',\n",
    "                   'Embarked']\n",
    "    original_dict = {}\n",
    "    encoder = LabelEncoder()\n",
    "    for i in col_for_lble:\n",
    "        train_data[i] = encoder.fit_transform(train_data[i])\n",
    "        original_dict[i] = dict(zip(encoder.classes_, encoder.transform(encoder.classes_)))\n",
    "        \n",
    "    return train_data, original_dict\n",
    "    \n",
    "\n",
    "def Preprocessing2(train_data):\n",
    "    #Drop columns that are not informative\n",
    "    train_data = train_data.drop(['Cabin','Name','Ticket','Ticket_number','Ticket_item','SexF','EmbarkedF','AgeR'],axis=1)\n",
    "    columns = [i for i in train_data.columns if i in ['Sex','Embarked','Pclass', 'Parch'] or 'cluster_' in i]\n",
    "               #+ ['Cabin','Name','Ticket','Ticket_number','Ticket_item']\n",
    "    #get_dumm = pd.get_dummies(train_data[columns], columns=columns)\n",
    "    train_data['weight'] = create_weight(train_data)\n",
    "    train_data = train_data.drop(columns +['PassengerId'], axis=1)\n",
    "    train_data['Age'] = round(train_data['Age'],2)\n",
    "    train_data['Fare'] = round(train_data['Fare'],3)\n",
    "    #Convert Dummy variables \n",
    "    return train_data\n",
    "\n",
    "def get_class_name(cls):\n",
    "    names = [base.__name__ for base in getmro(cls)]\n",
    "    return names[0]\n",
    "\n",
    "def is_subset(lst1, comb_list):\n",
    "    try_list = []\n",
    "    for i in comb_list:\n",
    "        if len(lst1) == len(i):\n",
    "            try_list.append(i)\n",
    "\n",
    "    search_len = len(lst1)\n",
    "    counts = []\n",
    "    for j in try_list:\n",
    "        count = 0\n",
    "        for elem in lst1:\n",
    "            if elem in j:\n",
    "                count+=1\n",
    "            else:\n",
    "                pass\n",
    "                counts.append(count)\n",
    "    if search_len in counts:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "    \n",
    "def create_combined_list(df):\n",
    "    import random\n",
    "    num_columns = df.shape[1]\n",
    "    combined_list = []\n",
    "    \n",
    "    number=5000\n",
    "    for _ in range(number):\n",
    "        num_cols = random.randint(2, 6)\n",
    "        indices = random.sample(range(num_columns), num_cols)\n",
    "        column_names = df.columns[indices].tolist()\n",
    "        combined_list.append(column_names)\n",
    "        if column_names in combined_list:\n",
    "            number+=1\n",
    "        if used_list(column_names) != 0:\n",
    "            number+=1\n",
    "        elif is_subset(column_names, combined_list) == False:\n",
    "            combined_list.remove(column_names)\n",
    "            number+=1\n",
    "    \n",
    "    return combined_list\n",
    "\n",
    "\n",
    "def find_best_ratio(dataframe):\n",
    "    best_coeff_1 = float('-inf')\n",
    "    best_coeff_2 = float('inf')\n",
    "    best_clusters = 0\n",
    "    \n",
    "    for index, row in dataframe.iterrows():\n",
    "        clusters = row['кластер']\n",
    "        coeff_1 = row['Калиский-Харабаср, больше лучше']\n",
    "        coeff_2 = row['Дэвид-Болдуин, меньше-лучше']\n",
    "        \n",
    "        if coeff_1 > best_coeff_1 and coeff_2 < best_coeff_2:\n",
    "            best_coeff_1 = coeff_1\n",
    "            best_coeff_2 = coeff_2\n",
    "            best_clusters = clusters\n",
    "    \n",
    "    return best_clusters\n",
    "\n",
    "def create_clusters(dict, df):\n",
    "    count = 0\n",
    "    for i, j in zip(dict.values(), dict.keys()):\n",
    "        model = KMeans(n_clusters=i[0],init ='random',n_init='auto')\n",
    "        model.fit(df[i[1]])\n",
    "        df[f'cluster_{j}'] = model.labels_\n",
    "#        if df[f'cluster_{j}'].corr(df['Survived'])*100 >= 3:\n",
    "#            pass\n",
    "#        else:\n",
    "#            df = df.drop([f'cluster_{j}'],axis=1)\n",
    "    return df\n",
    "\n",
    "def used_list(list):    \n",
    "    with open(r'C:\\Users\\aleksandrovva1\\Desktop\\titanic\\used_pairs.txt', 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        found_values = []\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line in str(list):\n",
    "                found_values.append(line)\n",
    "    return len(found_values)\n",
    "\n",
    "def find_cluster_list(dict_: dict):\n",
    "    find = []\n",
    "    for i in list(dict_.values()):\n",
    "        find.append(i[1])\n",
    "    return find\n",
    "\n",
    "\n",
    "def create_weight(df):\n",
    "    df['weight'] = df.apply(lambda x: assign_weight(x['Pclass']), axis=1)\n",
    "    return df['weight']\n",
    "\n",
    "def assign_weight(pclass):\n",
    "    if pclass == 3:\n",
    "        return 1\n",
    "    elif pclass == 2:\n",
    "        return 2\n",
    "    elif pclass == 1:\n",
    "        return 3\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "def munge_data(data, digest):\n",
    "    # Age - замена пропусков на медиану в зависимости от пола\n",
    "    data[\"AgeF\"] = data.apply(lambda r: digest.ages[r[\"Sex\"]] if pd.isnull(r[\"Age\"]) else r[\"Age\"], axis=1)\n",
    "\n",
    "    # Fare - замена пропусков на медиану в зависимости от класса\n",
    "    data[\"FareF\"] = data.apply(lambda r: digest.fares[r[\"Pclass\"]] if pd.isnull(r[\"Fare\"]) else r[\"Fare\"], axis=1)\n",
    "\n",
    "    # Gender - замена\n",
    "    genders = {\"male\": 1, \"female\": 0}\n",
    "    data[\"SexF\"] = data[\"Sex\"].apply(lambda s: genders.get(s))\n",
    "\n",
    "    # Gender - расширение\n",
    "    gender_dummies = pd.get_dummies(data[\"Sex\"], prefix=\"SexD\", dummy_na=False)\n",
    "    data = pd.concat([data, gender_dummies], axis=1)\n",
    "\n",
    "    # Embarkment - замена\n",
    "    embarkments = {\"U\": 0, \"S\": 1, \"C\": 2, \"Q\": 3}\n",
    "    data[\"EmbarkedF\"] = data[\"Embarked\"].fillna(\"U\").apply(lambda e: embarkments.get(e))\n",
    "\n",
    "    # Embarkment - расширение\n",
    "    embarkment_dummies = pd.get_dummies(data[\"Embarked\"], prefix=\"EmbarkedD\", dummy_na=False)\n",
    "    data = pd.concat([data, embarkment_dummies], axis=1)\n",
    "\n",
    "    # Количество родственников на борту\n",
    "    data[\"RelativesF\"] = data[\"Parch\"] + data[\"SibSp\"]\n",
    "\n",
    "    # Человек-одиночка?\n",
    "    data[\"SingleF\"] = data[\"RelativesF\"].apply(lambda r: 1 if r == 0 else 0)\n",
    "\n",
    "    # Deck - замена\n",
    "    decks = {\"U\": 0, \"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\": 5, \"F\": 6, \"G\": 7, \"T\": 8}\n",
    "    data[\"DeckF\"] = data[\"Cabin\"].fillna(\"U\").apply(lambda c: decks.get(c[0], -1))\n",
    "\n",
    "    # Deck - расширение\n",
    "    deck_dummies = pd.get_dummies(data[\"Cabin\"].fillna(\"U\").apply(lambda c: c[0]), prefix=\"DeckD\", dummy_na=False)\n",
    "    data = pd.concat([data, deck_dummies], axis=1)\n",
    "\n",
    "    # Titles - расширение\n",
    "    title_dummies = pd.get_dummies(data[\"Name\"].apply(lambda n: get_title(n)), prefix=\"TitleD\", dummy_na=False)\n",
    "    data = pd.concat([data, title_dummies], axis=1)\n",
    "\n",
    "    # амена текстов на индекс из соответствующего справочника или -1 если значения в справочнике нет (расширять не будем)\n",
    "    data[\"CabinF\"] = data[\"Cabin\"].fillna(\"unknown\").apply(lambda c: get_index(c, digest.cabins))\n",
    "\n",
    "    data[\"TitleF\"] = data[\"Name\"].apply(lambda n: get_index(get_title(n), digest.titles))\n",
    "\n",
    "    data[\"TicketF\"] = data[\"Ticket\"].apply(lambda t: get_index(t, digest.tickets))\n",
    "\n",
    "    data[\"FamilyF\"] = data.apply(lambda r: get_index(get_family(r), digest.families), axis=1)\n",
    "\n",
    "    # для статистики\n",
    "    age_bins = [0, 5, 10, 15, 20, 25, 30, 40, 50, 60, 70, 80, 90]\n",
    "    data[\"AgeR\"] = pd.cut(data[\"Age\"].fillna(-1), bins=age_bins).astype(object)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "0777c5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(r'C:\\Users\\aleksandrovva1\\Desktop\\titanic\\test.csv')\n",
    "train = pd.read_csv(r'C:\\Users\\aleksandrovva1\\Desktop\\titanic\\train.csv')\n",
    "passage = test['PassengerId']\n",
    "all_data = pd.concat([train, test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d6ebb4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_list = {0: [5, ['Sex', 'Embarked'], -54.77049946777053],\n",
    " 1: [12, ['Sex', 'Parch'], -54.47400538579452],\n",
    " 6: [11, ['Parch', 'Sex'], 53.33756781517999],\n",
    " 9: [14, ['SibSp', 'Pclass', 'Sex'], -55.12692610004996],\n",
    " 10: [5, ['Sex', 'Pclass'], 50.745756738870405],\n",
    " 13: [14, ['Embarked', 'Parch', 'Sex'], 51.29933341612405],\n",
    " 17: [10, ['Parch', 'Sex', 'SibSp'], 50.168970398268364]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "7471735d",
   "metadata": {},
   "outputs": [],
   "source": [
    "court = 0\n",
    "preproc_train = preposition(train)\n",
    "preproc_test = preposition(test)\n",
    "preproc_train, dict_for_train = Preprocessing(preproc_train)\n",
    "preproc_test, dict_for_test= Preprocessing(preproc_test)\n",
    "\n",
    "preproc_train = munge_data(preproc_train, data_digest)\n",
    "preproc_train = Preprocessing2(preproc_train)\n",
    "preproc_test = munge_data(preproc_test, data_digest)\n",
    "preproc_test = Preprocessing2(preproc_test)\n",
    "all_data_munged = pd.concat([preproc_train, preproc_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "5564f90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = [\"Pclass\",\n",
    "              \"AgeF\",\n",
    "              \"TitleF\",\n",
    "              \"TitleD_mr\", \"TitleD_mrs\", \"TitleD_miss\", \"TitleD_master\", \"TitleD_ms\", \n",
    "              \"TitleD_col\", \"TitleD_rev\", \"TitleD_dr\",\n",
    "              \"CabinF\",\n",
    "              \"DeckF\",\n",
    "              \"DeckD_U\", \"DeckD_A\", \"DeckD_B\", \"DeckD_C\", \"DeckD_D\", \"DeckD_E\", \"DeckD_F\", \"DeckD_G\",\n",
    "              \"FamilyF\",\n",
    "              \"TicketF\",\n",
    "              \"SexF\",\n",
    "              \"SexD_male\", \"SexD_female\",\n",
    "              \"EmbarkedF\",\n",
    "              \"EmbarkedD_S\", \"EmbarkedD_C\", \"EmbarkedD_Q\",\n",
    "              \"FareF\",\n",
    "              \"SibSp\", \"Parch\",\n",
    "              \"RelativesF\",\n",
    "              \"SingleF\"]\n",
    "target = 'Survived'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "8218eb06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Survived', 'Age', 'SibSp', 'Fare', 'AgeF', 'FareF', 'SexD_0', 'SexD_1',\n",
       "       'EmbarkedD_0', 'EmbarkedD_1', 'EmbarkedD_2', 'RelativesF', 'SingleF',\n",
       "       'DeckF', 'DeckD_A', 'DeckD_B', 'DeckD_C', 'DeckD_D', 'DeckD_E',\n",
       "       'DeckD_F', 'DeckD_G', 'DeckD_T', 'DeckD_U', 'TitleD_None', 'CabinF',\n",
       "       'TitleF', 'TicketF', 'FamilyF', 'weight'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preproc_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "719fa9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fff = create_clusters(unique_list, preproc_train)\n",
    "#preproc_train = Preprocessing2(preproc_train)\n",
    "columns_for_change = [i for i in preproc_train if i not in ['Survived','Age','Fare','Pclass_','Sex_', 'SexF','EmbarkedF'] \n",
    "                      and 'Sex_' not in i \n",
    "                      and 'Pclass_' not in i]\n",
    "#for column in columns_for_change:\n",
    "#    preproc_train[column+'_weight'] = preproc_train[column]/preproc_train['weight']\n",
    "preproc_train = preproc_train.drop(['DeckD_T'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "df410418",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_for_train_model = [i for i in preproc_train.columns if i not in ['Survived']]\n",
    "target = 'Survived'   \n",
    "train_x, test_x, train_y, test_y = train_test_split(preproc_train[columns_for_train_model],\n",
    "                                                    preproc_train[target], \n",
    "                                                    random_state=42, \n",
    "                                                    train_size=0.7,\n",
    "                                                    shuffle=True,\n",
    "                                                    stratify=preproc_train[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "eab03b90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48d21ef118d348adb223ebd0f4a49c77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MetricVisualizer(layout=Layout(align_self='stretch', height='500px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.8594586643130333"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = np.unique(train_y)\n",
    "weights = compute_class_weight(class_weight='balanced', classes=classes, y=train_y)\n",
    "class_weights = dict(zip(classes, weights))\n",
    "model = CatBoostClassifier(iterations=250,\n",
    "                           class_weights=class_weights,\n",
    "                           depth=2, \n",
    "                           verbose=False, \n",
    "                           learning_rate=0.02,\n",
    "                           use_best_model=True, \n",
    "                           leaf_estimation_method='Newton', \n",
    "                           l2_leaf_reg=0.3, \n",
    "                           border_count=13,\n",
    "                           eval_metric='AUC'\n",
    "                              )\n",
    "model.fit(train_x, train_y,\n",
    "         eval_set=(train_x, train_y),\n",
    "         plot=True)\n",
    "pred = model.predict_proba(test_x)[:,1]\n",
    "roc_auc_score(test_y, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f17a9b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [RandomForestClassifier, LogisticRegression, DecisionTreeClassifier, CatBoostClassifier, SVC, AdaBoostClassifier]\n",
    "scores = {}\n",
    "for j in range(1,15,1):\n",
    "    for i in models:\n",
    "        if i == CatBoostClassifier:\n",
    "            model = i(verbose=0).fit(train_x, train_y)\n",
    "        else:\n",
    "            model = i().fit(train_x, train_y)\n",
    "        pred = model.predict(test_x)\n",
    "        scores.setdefault(get_class_name(i), []).append(roc_auc_score(pred, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0d8c471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "расхождение по обучению по колонке RandomForestClassifier : 0.009436554535893394\n",
      "среднее по обучению по колонке RandomForestClassifier : 0.7956198804617794\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "расхождение по обучению по колонке LogisticRegression : 1.1102230246251565e-16\n",
      "среднее по обучению по колонке LogisticRegression : 0.787827715355805\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "расхождение по обучению по колонке DecisionTreeClassifier : 0.0053969340639694035\n",
      "среднее по обучению по колонке DecisionTreeClassifier : 0.7473127866662843\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "расхождение по обучению по колонке CatBoostClassifier : 0.0\n",
      "среднее по обучению по колонке CatBoostClassifier : 0.7976010101010101\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "расхождение по обучению по колонке SVC : 1.1102230246251565e-16\n",
      "среднее по обучению по колонке SVC : 0.6266233766233765\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "расхождение по обучению по колонке AdaBoostClassifier : 0.0016660160549596629\n",
      "среднее по обучению по колонке AdaBoostClassifier : 0.8067919990853036\n",
      "********************************************************************************\n"
     ]
    }
   ],
   "source": [
    "for i in pd.DataFrame(scores).columns:\n",
    "    print('****'*20)\n",
    "    print(f'расхождение по обучению по колонке {i} : {np.std(pd.DataFrame(scores)[i])}')\n",
    "    print(f'среднее по обучению по колонке {i} : {np.mean(pd.DataFrame(scores)[i])}')\n",
    "    print('****'*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "b1a8c440",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels = np.arange(0.01,1,0.01)\n",
    "scores = []\n",
    "\n",
    "for i in kernels:\n",
    "    model = LogisticRegression(\n",
    "    max_iter = 7,\n",
    "    C=13.8,\n",
    "    tol = 0.001,\n",
    "    solver = 'liblinear',\n",
    "    penalty = 'l2'\n",
    "    ).fit(train_x, train_y)\n",
    "    preds = model.predict_proba(test_x)[:,1]\n",
    "    scores.append(roc_auc_score(test_y, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "3959778e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(kernels,scores, c='red')\n",
    "#plt.xlim([5,20])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "59124336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC    : 0.7663724624889673\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a5ff73f0a9b46109234c7f7af48d67a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MetricVisualizer(layout=Layout(align_self='stretch', height='500px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoostClassifier    : 0.8650485436893205\n",
      "LogisticRegression    : 0.8561047366872611\n",
      "RandomForestClassifier    : 0.8585760517799353\n",
      "DecisionTreeClassifier    : 0.8297734627831715\n"
     ]
    }
   ],
   "source": [
    "svc = SVC(\n",
    "    C=4.4,\n",
    "    coef0 = 0.79,\n",
    "    class_weight=class_weights,\n",
    "    degree = 5,\n",
    "    probability=True\n",
    ").fit(train_x, train_y)\n",
    "pred = svc.predict_proba(test_x)[:,1]\n",
    "print(type(svc).__name__, '   :',roc_auc_score(test_y, pred))\n",
    "\n",
    "\n",
    "catboost = CatBoostClassifier(iterations=250,\n",
    "                           class_weights=class_weights,\n",
    "                           depth=2, \n",
    "                           verbose=False, \n",
    "                           learning_rate=0.02,\n",
    "                           use_best_model=True, \n",
    "                           leaf_estimation_method='Newton', \n",
    "                           l2_leaf_reg=0.3, \n",
    "                           border_count=13,\n",
    "                           eval_metric='AUC'\n",
    "                              )\n",
    "catboost.fit(train_x, train_y,\n",
    "         eval_set=(train_x, train_y),\n",
    "         plot=True)\n",
    "pred = catboost.predict_proba(test_x)[:,1]\n",
    "print(type(catboost).__name__, '   :',roc_auc_score(test_y, pred))\n",
    "\n",
    "logistic = LogisticRegression(\n",
    "    class_weight=class_weights,\n",
    "    max_iter = 7,\n",
    "    C=13.8,\n",
    "    tol = 0.001,\n",
    "    solver = 'liblinear',\n",
    "    penalty = 'l2'\n",
    "    ).fit(train_x, train_y)\n",
    "pred = logistic.predict_proba(test_x)[:,1]\n",
    "print(type(logistic).__name__, '   :',roc_auc_score(test_y, pred))\n",
    "\n",
    "rfr = RandomForestClassifier(\n",
    "    class_weight=class_weights,\n",
    "    bootstrap= False,\n",
    "    criterion= 'log_loss',\n",
    "    max_depth= 7,\n",
    "    min_samples_leaf= 24,\n",
    "    min_samples_split= 24,\n",
    "    n_estimators= 40\n",
    ").fit(train_x, train_y)\n",
    "pred = rfr.predict_proba(test_x)[:,1]\n",
    "print(type(rfr).__name__, '   :',roc_auc_score(test_y, pred))\n",
    "\n",
    "destree = DecisionTreeClassifier(\n",
    "    class_weight=class_weights,\n",
    "    ccp_alpha= 0.01,\n",
    "    criterion= 'gini',\n",
    "    max_depth= 7,\n",
    "    min_samples_leaf= 17,\n",
    "    min_samples_split= 17,\n",
    "    splitter= 'best'\n",
    ").fit(train_x, train_y)\n",
    "pred = destree.predict_proba(test_x)[:,1]\n",
    "print(type(destree).__name__, '   :',roc_auc_score(test_y, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f160be56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': False,\n",
       " 'criterion': 'log_loss',\n",
       " 'max_depth': 7,\n",
       " 'min_samples_leaf': 24,\n",
       " 'min_samples_split': 24,\n",
       " 'n_estimators': 40}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfr_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "940ecd3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tol': 0.1, 'max_iter': 91, 'C': 0.5}\n",
      "0.8449340305247891\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'max_iter' : np.arange(1,100,1),\n",
    "    'C':np.arange(0.1,30,0.1),\n",
    "    'tol': [0.001,0.01,0.1,0.0001,0.00001,0.000001,0.0000001,0.00000001]\n",
    "           }\n",
    "\n",
    "classes = np.unique(train_y)\n",
    "weights = compute_class_weight(class_weight='balanced', classes=classes, y=train_y)\n",
    "class_weights = dict(zip(classes, weights))\n",
    "\n",
    "model = LogisticRegression(class_weight=class_weights, \n",
    "                           solver = 'liblinear',\n",
    "                           penalty = 'l2')\n",
    "\n",
    "rs = RandomizedSearchCV(model, params, cv=3, random_state=42, scoring='roc_auc', n_iter=300).fit(train_x, train_y)\n",
    "print(rs.best_params_)\n",
    "print(rs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "18c84894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 10000/10000 [7:37:36<00:00,  2.75s/trial, best loss: -0.8283582089552238]\n"
     ]
    }
   ],
   "source": [
    "def objective(params):\n",
    "    model = MLPClassifier(**params)\n",
    "    model.fit(train_x, train_y)\n",
    "    pred = model.predict(test_x)\n",
    "    score = accuracy_score(test_y, pred)\n",
    "    return {'loss': -score, 'params':params, 'status': STATUS_OK}\n",
    "\n",
    "\n",
    "search_space = {\n",
    "    'hidden_layer_sizes': hp.choice('hidden_layer_sizes', [(100,), (100,100), (100,100,100), (100,100,100,100)]),\n",
    "    'activation': hp.choice('activation', ['identity', 'logistic', 'tanh', 'relu']),\n",
    "    'alpha': hp.choice('alpha', [0.1,0.01,0.001,0.0001,0.00001,0.000001,0.0000001]),\n",
    "    'learning_rate': hp.choice('learning_rate', ['constant', 'invscaling', 'adaptive']),\n",
    "    'max_iter': hp.choice('max_iter', np.arange(100,10000,100)),\n",
    "    'batch_size': hp.choice('batch_size', np.arange(10,150, 10)),\n",
    "    'validation_fraction': hp.choice('validation_fraction', np.arange(0,1,0.01)),\n",
    "    'beta_1': hp.choice('beta_1', np.arange(0.01,1,0.01)),\n",
    "    'beta_2': hp.choice('beta_2', np.arange(0.01,1,0.01))\n",
    "}\n",
    "\n",
    "\n",
    "trial_neural = Trials()\n",
    "best = fmin(fn=objective, \n",
    "            space=search_space, \n",
    "            algo = tpe.suggest,\n",
    "            max_evals=10000,\n",
    "            trials=trial_neural)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "9c992ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 10000/10000 [45:02<00:00,  3.70trial/s, best loss: -0.8395522388059702]\n"
     ]
    }
   ],
   "source": [
    "def objective(params):\n",
    "    model = xgb.XGBClassifier(**params)\n",
    "    model.fit(train_x, train_y)\n",
    "    pred = model.predict(test_x)\n",
    "    score = accuracy_score(test_y, pred)\n",
    "    return {'loss': -score, 'params':params, 'status': STATUS_OK}\n",
    "\n",
    "\n",
    "search_space = {\n",
    "    'learning_rate': hp.choice('learning_rate', np.arange(0.01,0.3,0.01)),\n",
    "    'n_estimators': hp.choice('n_estimators', np.arange(100,1000,100)),\n",
    "    'max_depth': hp.choice('max_depth', np.arange(1,10,1)),\n",
    "    'min_child_weight': hp.choice('min_child_weight', np.arange(1,10,1)),\n",
    "    'gamma': hp.choice('gamma', np.arange(0.01,1,0.01)),\n",
    "    'subsample': hp.choice('subsample', np.arange(0.5,1, 0.1)),\n",
    "    'colsample_bytree': hp.choice('colsample_bytree ', np.arange(0.5,1, 0.1))\n",
    "}\n",
    "\n",
    "\n",
    "trial_xgb = Trials()\n",
    "best = fmin(fn=objective, \n",
    "            space=search_space, \n",
    "            algo = tpe.suggest,\n",
    "            max_evals=10000,\n",
    "            trials=trial_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "0970bf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "xbb = xgb.XGBClassifier(**xgb_params).fit(preproc_train[columns_for_train_model], preproc_train[target])\n",
    "pred = xbb.predict(preproc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "7275ce9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(**neural_params).fit(preproc_train[columns_for_train_model], preproc_train[target])\n",
    "pred = mlp.predict(preproc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "93953f16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': -0.8395522388059702,\n",
       " 'params': {'colsample_bytree': 0.6,\n",
       "  'gamma': 0.5800000000000001,\n",
       "  'learning_rate': 0.06999999999999999,\n",
       "  'max_depth': 2,\n",
       "  'min_child_weight': 1,\n",
       "  'n_estimators': 500,\n",
       "  'subsample': 0.8999999999999999},\n",
       " 'status': 'ok'}"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_xgb.best_trial['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "80eb2b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = trial_xgb.best_trial['result']['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "13f3c46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_params = trial_neural.best_trial['result']['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315c8dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "    model = MLPClassifier(**params)\n",
    "    model.fit(train_x, train_y)\n",
    "    pred = model.predict(test_x)\n",
    "    score = accuracy_score(test_y, pred)\n",
    "    return {'loss': -score, 'params':params, 'status': STATUS_OK}\n",
    "\n",
    "\n",
    "search_space = {\n",
    "    'hidden_layer_sizes': hp.choice('hidden_layer_sizes', [(100,), (100,100), (100,100,100), (100,100,100,100)]),\n",
    "    'activation': hp.choice('activation', ['identity', 'logistic', 'tanh', 'relu']),\n",
    "    'alpha': hp.choice('alpha', [0.1,0.01,0.001,0.0001,0.00001,0.000001,0.0000001]),\n",
    "    'learning_rate': hp.choice('learning_rate', ['constant', 'invscaling', 'adaptive']),\n",
    "    'max_iter': hp.choice('max_iter', np.arange(100,10000,100)),\n",
    "    'batch_size': hp.choice('batch_size', np.arange(10,150, 10)),\n",
    "    'validation_fraction': hp.choice('validation_fraction', np.arange(0,1,0.01)),\n",
    "    'beta_1': hp.choice('beta_1', np.arange(0.01,1,0.01)),\n",
    "    'beta_2': hp.choice('beta_2', np.arange(0.01,1,0.01))\n",
    "}\n",
    "\n",
    "\n",
    "trial = Trials()\n",
    "best = fmin(fn=objective, \n",
    "            space=search_space, \n",
    "            algo = tpe.suggest,\n",
    "            max_evals=10000,\n",
    "            trials=trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5ba04032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activation = logistic ,\n",
      "alpha = 0.1 ,\n",
      "batch_size = 140 ,\n",
      "hidden_layer_sizes = (100, 100, 100, 100) ,\n",
      "learning_rate = adaptive ,\n",
      "max_iter = 2200 ,\n",
      "validation_fraction = 0.19 ,\n"
     ]
    }
   ],
   "source": [
    "for i,j in trial.best_trial['result']['params'].items():\n",
    "    print(i,'=',j,',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ccd163d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = trial.best_trial['result']['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859dc34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = VotingClassifier(estimators=[('svc', svc), ('logistic', logistic), ('rfr', rfr), ('destree', destree),\n",
    "                                       ('catboost', catboost)], voting='soft').fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1022ee6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8636363636363635"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = MLPClassifier(**params).fit(train_x, train_y)\n",
    "pred = mlp.predict_proba(test_x)[:,1]\n",
    "roc_auc_score(test_y, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d3fba744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'logistic',\n",
       " 'alpha': 0.1,\n",
       " 'batch_size': 140,\n",
       " 'beta_1': 0.9,\n",
       " 'beta_2': 0.999,\n",
       " 'early_stopping': False,\n",
       " 'epsilon': 1e-08,\n",
       " 'hidden_layer_sizes': (100, 100, 100, 100),\n",
       " 'learning_rate': 'adaptive',\n",
       " 'learning_rate_init': 0.001,\n",
       " 'max_fun': 15000,\n",
       " 'max_iter': 2200,\n",
       " 'momentum': 0.9,\n",
       " 'n_iter_no_change': 10,\n",
       " 'nesterovs_momentum': True,\n",
       " 'power_t': 0.5,\n",
       " 'random_state': None,\n",
       " 'shuffle': True,\n",
       " 'solver': 'adam',\n",
       " 'tol': 0.0001,\n",
       " 'validation_fraction': 0.19,\n",
       " 'verbose': False,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2ddccf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "params['beta_1'] = 0.22\n",
    "params['beta_2'] = 0.21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f801b927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 2000/2000 [56:44<00:00,  1.70s/trial, best loss: -0.8101206237128569]\n"
     ]
    }
   ],
   "source": [
    "beta1 = np.arange(0.1,1.1,0.1)\n",
    "beta2 = np.arange(0.01,1.01,0.01)\n",
    "def objective(params):\n",
    "    model = MLPClassifier(**params,\n",
    "                         activation = 'logistic' ,\n",
    "                         alpha = 0.1 ,\n",
    "                         batch_size = 140 ,\n",
    "                         hidden_layer_sizes = (100, 100, 100, 100) ,\n",
    "                         learning_rate = 'adaptive' ,\n",
    "                         max_iter = 2200 ,\n",
    "                         validation_fraction = 0.19)\n",
    "    model.fit(train_x, train_y)\n",
    "    pred = model.predict(test_x)\n",
    "    score = roc_auc_score(test_y, pred)\n",
    "    return {'loss': -score, 'params':params, 'status': STATUS_OK}\n",
    "\n",
    "\n",
    "search_space = {\n",
    "    'beta_1': hp.choice('beta_1', np.arange(0.01,1,0.01)),\n",
    "    'beta_2': hp.choice('beta_2', np.arange(0.01,1,0.01))\n",
    "}\n",
    "\n",
    "\n",
    "trial1 = Trials()\n",
    "best = fmin(fn=objective, \n",
    "            space=search_space, \n",
    "            algo = tpe.suggest,\n",
    "            max_evals=2000,\n",
    "            trials=trial1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af60f173",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b57c913a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'beta_1': 0.22, 'beta_2': 0.21000000000000002}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial1.best_trial['result']['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1a62f73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(**params).fit(fff[columns_for_train_model], fff[target])\n",
    "#pred = mlp.predict_proba(test_x)[:,1]\n",
    "#roc_auc_score(test_y, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e3b05ed6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.81564567, 0.87208801, 0.88009299])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(CatBoostClassifier(iterations=250,\n",
    "                           class_weights=class_weights,\n",
    "                           depth=2, \n",
    "                           verbose=False, \n",
    "                           learning_rate=0.02, \n",
    "                           leaf_estimation_method='Newton', \n",
    "                           l2_leaf_reg=0.3, \n",
    "                           border_count=13\n",
    "                              ), fff[columns_for_train_model], fff[target], scoring='roc_auc', cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a4587602",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = mlp.predict(preproc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "7aaeffeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = pd.DataFrame({\n",
    "    'PassengerId': passage,\n",
    "    'Survived': pred\n",
    "})\n",
    "predict.to_csv(r'C:\\Users\\aleksandrovva1\\Desktop\\titanic\\gender_submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "34ccd532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "974b4e05f6044bfab0b37ded3a0c8045",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MetricVisualizer(layout=Layout(align_self='stretch', height='500px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x23020bcff40>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catboost = CatBoostClassifier(iterations=250,\n",
    "                           class_weights=class_weights,\n",
    "                           depth=2, \n",
    "                           verbose=False, \n",
    "                           learning_rate=0.02,\n",
    "                           use_best_model=True, \n",
    "                           leaf_estimation_method='Newton', \n",
    "                           l2_leaf_reg=0.3, \n",
    "                           border_count=13,\n",
    "                           eval_metric='AUC'\n",
    "                              )\n",
    "catboost.fit(fff[columns_for_train_model], fff[target],eval_set=(fff[columns_for_train_model], fff[target]), plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b7193e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = catboost.predict(preproc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "74d8d129",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = pd.DataFrame({\n",
    "    'PassengerId': passage,\n",
    "    'Survived': pred\n",
    "})\n",
    "predict.to_csv(r'C:\\Users\\aleksandrovva1\\Desktop\\titanic\\gender_submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76fa1a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
